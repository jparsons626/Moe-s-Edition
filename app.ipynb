{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rc5ra_T1kws0"
      },
      "outputs": [],
      "source": [
        "# History_Bonds_Target_v4.3.1.py\n",
        "# - Cleaned workbook (unchanged)\n",
        "# - Target workbook (unchanged from 4.3)\n",
        "# - Event Analysis:\n",
        "#     * Sheet names = UST buckets (unique-suffixed)\n",
        "#     * Axis dates PER BOND: min = first non-NaN date for that bond; max = last non-NaN date\n",
        "#     * Chart 1: Prior solid line; Post square markers (same color), no connecting line\n",
        "#     * Chart 2: Post-only; x-axis min = max(event_date, bond first non-NaN); max = last non-NaN\n",
        "# - Output folder: Output {M-D-YYYY}\n",
        "# - Colab auto-downloads: Cleaned/Target/Event at the end\n",
        "\n",
        "import os, re, glob, shutil, math, subprocess\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, date, timedelta, time as dtime\n",
        "from collections import defaultdict\n",
        "\n",
        "# ---- Globals so the download block can see what was written ----\n",
        "OUT_XLSX = None\n",
        "TARGET_XLSX = None\n",
        "EVENT_XLSX = None\n",
        "\n",
        "# ---- Ensure xlsxwriter quietly ----\n",
        "try:\n",
        "    import xlsxwriter  # noqa: F401\n",
        "except Exception:\n",
        "    subprocess.run([\"pip\", \"install\", \"-q\", \"xlsxwriter\"], check=False)\n",
        "    import xlsxwriter  # noqa: F401\n",
        "\n",
        "# ---- Time/date + folders ----\n",
        "try:\n",
        "    from zoneinfo import ZoneInfo\n",
        "    tz = ZoneInfo(\"America/New_York\")\n",
        "except Exception:\n",
        "    tz = None\n",
        "\n",
        "now = datetime.now(tz) if tz else datetime.now()\n",
        "try:\n",
        "    FS_DATE = now.strftime(\"%-m-%-d-%Y\")\n",
        "except ValueError:\n",
        "    FS_DATE = now.strftime(\"%#m-%#d-%Y\")\n",
        "\n",
        "RAW_DIR    = f\"Raw {FS_DATE}\"\n",
        "OUTPUT_DIR = f\"Output {FS_DATE}\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# =========================\n",
        "# Common helpers\n",
        "# =========================\n",
        "COUPON_DECIMALS = 3\n",
        "def _to_float_safe(x):\n",
        "    try: return float(str(x).replace(\",\", \"\").strip())\n",
        "    except Exception: return np.nan\n",
        "def _format_coupon(c):\n",
        "    if pd.isna(c): return \"\"\n",
        "    val = round(float(c), COUPON_DECIMALS)\n",
        "    return f\"{val:.{COUPON_DECIMALS}f}\".rstrip(\"0\").rstrip(\".\")\n",
        "LABEL_RE = re.compile(r\"^\\s*(?P<ticker>[A-Za-z0-9\\.\\-]+)\\s+(?P<coupon>\\d+(?:\\.\\d+)?)%\\s+(?P<month>\\d{1,2})[\\/\\-](?P<year>\\d{2,4})\\s*$\")\n",
        "def parse_bond_label(label: str | None):\n",
        "    if not isinstance(label, str): return None\n",
        "    s = re.sub(r\"\\s+(T[- ]?Spread|G[- ]?Spread|Confidence)\\s*$\", \"\", label.strip(), flags=re.IGNORECASE)\n",
        "    s = re.sub(r\"(\\b\\d{1,2})-(\\d{2,4}\\b)\", r\"\\1/\\2\", s)\n",
        "    m = LABEL_RE.match(s)\n",
        "    if not m: return None\n",
        "    d = m.groupdict()\n",
        "    y = int(d[\"year\"])\n",
        "    year = y + (2000 if y < 80 else (0 if y >= 1900 else 1900))\n",
        "    return {\"ticker\": d[\"ticker\"].upper(), \"coupon\": _to_float_safe(d[\"coupon\"]), \"month\": int(d[\"month\"]), \"year\": year}\n",
        "def canonical_label(d):\n",
        "    if not d: return None\n",
        "    return f'{d[\"ticker\"]} {_format_coupon(d[\"coupon\"])}% {d[\"month\"]}/{d[\"year\"]}'\n",
        "def norm_ust(u):\n",
        "    s = str(u).strip()\n",
        "    if not s: return s\n",
        "    if s.lower().startswith(\"perp\"): return \"Perp\"\n",
        "    m = re.match(r\"(\\d+)\", s)\n",
        "    return (m.group(1) + \"y\") if m else s\n",
        "\n",
        "def months_between_inclusive(a: date, b: date) -> int:\n",
        "    if a > b: a, b = b, a\n",
        "    return (b.year - a.year) * 12 + (b.month - a.month) + 1\n",
        "\n",
        "def configure_date_axis_monthly(chart, start_d: date, end_d: date):\n",
        "    month_count = months_between_inclusive(start_d, end_d)\n",
        "    major_unit = 2 if month_count >= 8 else 1\n",
        "    chart.set_x_axis({\n",
        "        'date_axis': True,\n",
        "        'num_format': \"mmm 'yy\",\n",
        "        'label_position': 'low',\n",
        "        'major_unit': major_unit,\n",
        "        'major_unit_type': 'months',\n",
        "        'min': datetime.combine(start_d, dtime.min),\n",
        "        'max': datetime.combine(end_d, dtime.min),\n",
        "        'num_font': {'rotation': 0},\n",
        "        'major_gridlines': {'visible': False},\n",
        "        'minor_gridlines': {'visible': False},\n",
        "    })\n",
        "\n",
        "def y_range_5s_push10(vals) -> tuple[float, float]:\n",
        "    arr = np.array(vals, dtype=float)\n",
        "    arr = arr[~np.isnan(arr)]\n",
        "    if arr.size == 0: return (0.0, 5.0)\n",
        "    ymin, ymax = float(np.nanmin(arr)), float(np.nanmax(arr))\n",
        "    down5 = lambda x: 5.0 * math.floor(x / 5.0)\n",
        "    up5   = lambda x: 5.0 * math.ceil(x / 5.0)\n",
        "    y_min, y_max = down5(ymin), up5(ymax)\n",
        "    if y_min == y_max: y_max = y_min + 5.0\n",
        "    if (y_max - y_min) % 10 != 0:\n",
        "        y_max += (10 - ((y_max - y_min) % 10))\n",
        "    return (y_min, y_max)\n",
        "\n",
        "def base_chart(wb):\n",
        "    ch = wb.add_chart({'type': 'line'})\n",
        "    ch.set_title({'name': ''})\n",
        "    ch.set_plotarea({'fill': {'none': True}, 'border': {'none': True}})\n",
        "    ch.set_chartarea({'fill': {'none': True}, 'border': {'none': True}})\n",
        "    ch.set_legend({'position': 'bottom'})\n",
        "    return ch\n",
        "\n",
        "# simple palette for Event Analysis\n",
        "EA_COLORS = [\"#1F497D\", \"#C0504D\", \"#9BBB59\", \"#8064A2\", \"#4BACC6\", \"#F79646\", \"#2F5597\", \"#A31515\"]\n",
        "\n",
        "# =========================\n",
        "# File ingestion\n",
        "# =========================\n",
        "def get_raw_folder() -> str:\n",
        "    resp = input(\"Do you want to upload new files? (y/n): \").strip().lower()\n",
        "    if resp in {\"y\", \"yes\"}:\n",
        "        if os.path.exists(RAW_DIR):\n",
        "            shutil.rmtree(RAW_DIR)\n",
        "        os.makedirs(RAW_DIR, exist_ok=True)\n",
        "        try:\n",
        "            from google.colab import files as colab_files\n",
        "            print(\"Please upload your Excel files now...\")\n",
        "            uploaded = colab_files.upload()\n",
        "            for fname, data in uploaded.items():\n",
        "                with open(os.path.join(RAW_DIR, fname), \"wb\") as f:\n",
        "                    f.write(data)\n",
        "            print(f\"âœ… Saved {len(uploaded)} file(s) into: {RAW_DIR}\")\n",
        "        except Exception as e:\n",
        "            raise SystemExit(f\"Upload failed: {e}\")\n",
        "        return RAW_DIR\n",
        "    else:\n",
        "        raw_folders = sorted(glob.glob(\"Raw *\"), key=os.path.getmtime)\n",
        "        if not raw_folders:\n",
        "            raise SystemExit(\"â— No Raw folders found. Please upload new data.\")\n",
        "        selected = raw_folders[0]\n",
        "        print(f\"Using existing folder: {selected}\")\n",
        "        return selected\n",
        "\n",
        "def ingest_bonds_and_history(raw_dir: str):\n",
        "    paths = sorted(glob.glob(os.path.join(raw_dir, \"*.xlsx\")))\n",
        "    if not paths:\n",
        "        raise SystemExit(\"â— No Excel files found in the selected Raw folder.\")\n",
        "\n",
        "    history_files, bonds_file = [], None\n",
        "    for path in paths:\n",
        "        try:\n",
        "            xl = pd.ExcelFile(path)\n",
        "        except Exception:\n",
        "            continue\n",
        "        if \"Combined Level History\" in xl.sheet_names:\n",
        "            history_files.append((path, xl))\n",
        "        else:\n",
        "            try:\n",
        "                df0 = pd.read_excel(xl, sheet_name=xl.sheet_names[0], header=0)\n",
        "            except Exception:\n",
        "                continue\n",
        "            if df0.shape[1] >= 22 and \"%\" in str(df0.iloc[0, 3]):\n",
        "                bonds_file = (path, xl)\n",
        "\n",
        "    # Bonds\n",
        "    bond_table_df = pd.DataFrame()\n",
        "    if bonds_file:\n",
        "        bonds_path, bonds_xl = bonds_file\n",
        "        raw = pd.read_excel(bonds_path, sheet_name=bonds_xl.sheet_names[0], header=0)\n",
        "        to_date = lambda s: pd.to_datetime(s, errors=\"coerce\").dt.date\n",
        "        bond_table_df = pd.DataFrame({\n",
        "            \"Issuer\":         raw.iloc[:, 2],\n",
        "            \"Bond\":           raw.iloc[:, 3].astype(str).str.replace(r\"(\\b\\d{1,2})-(\\d{4}\\b)\", r\"\\1/\\2\", regex=True).str.strip(),\n",
        "            \"Seniority\":      raw.iloc[:, 10],\n",
        "            \"Moody's\":        raw.iloc[:, 11],\n",
        "            \"S&P Rating\":     \"\",\n",
        "            \"Issue Date\":     to_date(raw.iloc[:, 5]),\n",
        "            \"Tenor\":          pd.to_numeric(raw.iloc[:, 7], errors=\"coerce\"),\n",
        "            \"Maturity\":       to_date(raw.iloc[:, 6]),\n",
        "            \"UST\":            raw.iloc[:, 8],\n",
        "            \"Coupon\":         pd.to_numeric(raw.iloc[:, 4], errors=\"coerce\"),\n",
        "            \"Amount Issued\":  pd.to_numeric(raw.iloc[:, 9], errors=\"coerce\"),\n",
        "            \"T-Spread\":       pd.to_numeric(raw.iloc[:, 14], errors=\"coerce\"),\n",
        "        })\n",
        "        bond_table_df[\"UST\"] = bond_table_df[\"UST\"].apply(norm_ust)\n",
        "\n",
        "    # Adjusted (history)\n",
        "    series_map = {}\n",
        "    for h_path, h_xl in history_files:\n",
        "        hist = pd.read_excel(h_path, sheet_name=\"Combined Level History\", header=0)\n",
        "        if hist.empty or hist.shape[1] < 2: continue\n",
        "        date_col = pd.to_datetime(hist.iloc[:, 0], errors=\"coerce\")\n",
        "        t_cols = [c for c in hist.columns if isinstance(c, str) and re.search(r\"\\bT[- ]?Spread\\b\", c, flags=re.IGNORECASE)]\n",
        "        for col in t_cols:\n",
        "            base_label = re.sub(r\"\\s*T[- ]?Spread\\s*$\", \"\", col, flags=re.IGNORECASE).strip()\n",
        "            base_label = re.sub(r\"(\\b\\d{1,2})-(\\d{4}\\b)\", r\"\\1/\\2\", base_label)\n",
        "            parsed = parse_bond_label(base_label)\n",
        "            canon = canonical_label(parsed) if parsed else base_label\n",
        "            s = pd.to_numeric(hist[col], errors=\"coerce\")\n",
        "            s.index = date_col\n",
        "            s = s[~s.index.isna()]\n",
        "            s = s[~s.index.duplicated(keep=\"last\")]\n",
        "            series_map[canon] = series_map.get(canon, pd.Series(dtype=float)).combine_first(s) if canon in series_map else s\n",
        "\n",
        "    if series_map:\n",
        "        all_dates = sorted(set().union(*[s.index for s in series_map.values()]))\n",
        "        adjusted_df = pd.DataFrame({\"Date\": pd.to_datetime(all_dates).date})\n",
        "        for col_name in sorted(series_map.keys()):\n",
        "            adjusted_df[col_name] = series_map[col_name].reindex(all_dates).values\n",
        "    else:\n",
        "        adjusted_df = pd.DataFrame(columns=[\"Date\"])\n",
        "\n",
        "    # TRACE (optional)\n",
        "    trace_frames = []\n",
        "    def _norm_party(x): return \"\" if pd.isna(x) else str(x).strip().upper()\n",
        "    for h_path, h_xl in history_files:\n",
        "        for sname in h_xl.sheet_names:\n",
        "            if not sname.startswith(\"Trades \"): continue\n",
        "            try:\n",
        "                df_tr = pd.read_excel(h_path, sheet_name=sname, header=0)\n",
        "            except Exception:\n",
        "                continue\n",
        "            if df_tr.empty or df_tr.shape[1] < 14: continue\n",
        "            date_ser      = pd.to_datetime(df_tr.iloc[:, 0], errors=\"coerce\").dt.date\n",
        "            qty_ser       = pd.to_numeric(df_tr.iloc[:, 1], errors=\"coerce\")\n",
        "            buysell_ser   = df_tr.iloc[:, 2].astype(str).str.strip()\n",
        "            tspread_ser   = pd.to_numeric(df_tr.iloc[:, 3], errors=\"coerce\")\n",
        "            price_ser     = pd.to_numeric(df_tr.iloc[:, 5], errors=\"coerce\")\n",
        "            reporting_ser = df_tr.iloc[:, 8].apply(_norm_party)\n",
        "            contra_ser    = df_tr.iloc[:, 9].apply(_norm_party)\n",
        "            mask = reporting_ser.isin([\"\", \"D\"]) & contra_ser.isin([\"\", \"C\"])\n",
        "            df_out = pd.DataFrame({\n",
        "                \"Date\": date_ser, \"Bond\": \"\", \"Quantity\": qty_ser, \"Buy/Sell\": buysell_ser,\n",
        "                \"T-Spread\": tspread_ser, \"Price\": price_ser,\n",
        "            })\n",
        "            df_out = df_out[mask & df_out[\"Date\"].notna()]\n",
        "            if not df_out.empty: trace_frames.append(df_out)\n",
        "    trace_df = pd.concat(trace_frames, ignore_index=True) if trace_frames else pd.DataFrame(\n",
        "        columns=[\"Date\",\"Bond\",\"Quantity\",\"Buy/Sell\",\"T-Spread\",\"Price\"]\n",
        "    )\n",
        "\n",
        "    return bond_table_df, adjusted_df, trace_df\n",
        "\n",
        "# =========================\n",
        "# CLEANED workbook\n",
        "# =========================\n",
        "def build_cleaned_workbook(bond_table_df: pd.DataFrame, adjusted_df: pd.DataFrame, trace_df: pd.DataFrame) -> str:\n",
        "    global OUT_XLSX\n",
        "    OUT_XLSX = os.path.join(OUTPUT_DIR, f\"Cleaned {FS_DATE}.xlsx\")\n",
        "    with pd.ExcelWriter(OUT_XLSX, engine=\"openpyxl\", datetime_format=\"yyyy-mm-dd\") as writer:\n",
        "        bond_table_df.to_excel(writer, sheet_name=\"Bond Table\", index=False)\n",
        "        adjusted_df.to_excel(writer, sheet_name=\"Adjusted\", index=False)\n",
        "        trace_df.to_excel(writer, sheet_name=\"TRACE\", index=False)\n",
        "    print(f\"ðŸ“˜ Wrote: {OUT_XLSX}\")\n",
        "    return OUT_XLSX\n",
        "\n",
        "# =========================\n",
        "# Utilities from Bond Table\n",
        "# =========================\n",
        "def _maps_from_bond_table(bond_table_df: pd.DataFrame):\n",
        "    canon_issue_map, canon_ust_map = {}, {}\n",
        "    if isinstance(bond_table_df, pd.DataFrame) and not bond_table_df.empty:\n",
        "        for _, r in bond_table_df[[\"Bond\",\"Issue Date\"]].dropna(subset=[\"Bond\"]).iterrows():\n",
        "            label = re.sub(r\"(\\b\\d{1,2})-(\\d{4}\\b)\", r\"\\1/\\2\", str(r[\"Bond\"]).strip())\n",
        "            parsed = parse_bond_label(label); canon = canonical_label(parsed) if parsed else None\n",
        "            if not canon: continue\n",
        "            iss_dt = pd.to_datetime(r[\"Issue Date\"], errors=\"coerce\")\n",
        "            if pd.isna(iss_dt): continue\n",
        "            prev = canon_issue_map.get(canon)\n",
        "            if prev is None or iss_dt > prev: canon_issue_map[canon] = iss_dt\n",
        "        for _, r in bond_table_df[[\"Bond\",\"UST\"]].dropna(subset=[\"Bond\"]).iterrows():\n",
        "            label = re.sub(r\"(\\b\\d{1,2})-(\\d{4}\\b)\", r\"\\1/\\2\", str(r[\"Bond\"]).strip())\n",
        "            parsed = parse_bond_label(label); canon = canonical_label(parsed) if parsed else None\n",
        "            if not canon: continue\n",
        "            canon_ust_map[canon] = norm_ust(r[\"UST\"])\n",
        "    return canon_issue_map, canon_ust_map\n",
        "\n",
        "# =========================\n",
        "# TARGET workbook (same charts logic as v4.3)\n",
        "# =========================\n",
        "def pick_month_window(dates: pd.Series, target_series: pd.Series, peer_avg_series: pd.Series) -> tuple[date, date]:\n",
        "    dts = pd.to_datetime(dates, errors=\"coerce\").dt.date\n",
        "    df = pd.DataFrame({\"Date\": dts, \"t\": pd.to_numeric(target_series, errors=\"coerce\"),\n",
        "                       \"pav\": pd.to_numeric(peer_avg_series, errors=\"coerce\")}).dropna(subset=[\"Date\"])\n",
        "    if df.empty:\n",
        "        today = date.today()\n",
        "        start = date(today.year, today.month, 1)\n",
        "        end   = (date(today.year + (1 if today.month==12 else 0), 1 if today.month==12 else today.month+1, 1) - timedelta(days=1))\n",
        "        return start, end\n",
        "    df[\"y\"] = [d.year for d in df[\"Date\"]]; df[\"m\"] = [d.month for d in df[\"Date\"]]\n",
        "    tgt_m = set(df.loc[df[\"t\"].notna(), [\"y\",\"m\"]].itertuples(index=False, name=None))\n",
        "    pav_m = set(df.loc[df[\"pav\"].notna(), [\"y\",\"m\"]].itertuples(index=False, name=None))\n",
        "    both = sorted(tgt_m & pav_m)\n",
        "    if both:\n",
        "        ys, ms = both[0]; start = date(ys, ms, 1)\n",
        "    else:\n",
        "        ft = sorted(tgt_m)[0] if tgt_m else None\n",
        "        fp = sorted(pav_m)[0] if pav_m else None\n",
        "        if ft and fp: yx, mx = max(ft, fp); start = date(yx, mx, 1)\n",
        "        elif ft:      yx, mx = ft;          start = date(yx, mx, 1)\n",
        "        elif fp:      yx, mx = fp;          start = date(yx, mx, 1)\n",
        "        else:         start = min(df[\"Date\"])\n",
        "    either = sorted(tgt_m | pav_m)\n",
        "    ye, me = either[-1]\n",
        "    end = date(ye, me, 1)\n",
        "    if me == 12: end = date(ye, 12, 31)\n",
        "    else:        end = date(ye, me+1, 1) - timedelta(days=1)\n",
        "    all_dates = set(df[\"Date\"])\n",
        "    if end not in all_dates:\n",
        "        fwd = [d for d in all_dates if d >= end]; back = [d for d in all_dates if d <= end]\n",
        "        if fwd: end = min(fwd)\n",
        "        elif back: end = max(back)\n",
        "    if start > end: start, end = min(all_dates), max(all_dates)\n",
        "    return start, end\n",
        "\n",
        "def _write_bond_table_target(writer, bond_table_df, target_by_ust):\n",
        "    # (same as in v4.3; omitted here for brevity in explanation)\n",
        "    cols = [\"Issuer\",\"Bond\",\"Seniority\",\"Moody's\",\"S&P Rating\",\"Issue Date\",\"Tenor\",\n",
        "            \"Maturity\",\"UST\",\"Coupon\",\"Quantity\",\"T-Spread\"]\n",
        "    df = bond_table_df.copy().rename(columns={\"Amount Issued\":\"Quantity\"})\n",
        "    df = df[cols]\n",
        "    df[\"Issue Date\"] = pd.to_datetime(df[\"Issue Date\"], errors=\"coerce\").dt.date\n",
        "    df[\"Maturity\"]   = pd.to_datetime(df[\"Maturity\"], errors=\"coerce\").dt.date\n",
        "    df[\"Tenor\"]      = pd.to_numeric(df[\"Tenor\"], errors=\"coerce\")\n",
        "    for c in [\"Coupon\",\"Quantity\",\"T-Spread\"]: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    def bond_left(s):\n",
        "        s = str(s) if pd.notna(s) else \"\"\n",
        "        m = re.search(r\"%\", s);  return s[:m.end()] if m else s.strip()\n",
        "    def fmt_m_yyyy(d):  return \"\" if pd.isna(d) or d is None else f\"{d.month}/{d.year}\"\n",
        "    df[\"Bond\"] = [f\"{bond_left(b)} {fmt_m_yyyy(m)}\".strip() for b, m in zip(df[\"Bond\"], df[\"Maturity\"])]\n",
        "    df[\"UST\"] = df[\"UST\"].apply(norm_ust)\n",
        "    def ust_key(u):\n",
        "        s = str(u)\n",
        "        if s == \"Perp\": return (10**9, s)\n",
        "        m = re.match(r\"(\\d+)\", s);  return (int(m.group(1)) if m else 10**8, s)\n",
        "    df[\"__ustkey\"] = df[\"UST\"].map(ust_key)\n",
        "    df.sort_values(by=[\"__ustkey\",\"Tenor\",\"Issuer\"], ascending=[True, True, True], inplace=True)\n",
        "    wb = writer.book\n",
        "    ws = wb.add_worksheet(\"Bond Table\")\n",
        "    ws.hide_gridlines(2)\n",
        "    blue = \"#1F497D\"\n",
        "    prehdr = wb.add_format({'bg_color': blue})\n",
        "    hdr = wb.add_format({'bold': True,'font_color': 'white','bg_color': blue,\n",
        "                         'font_name': 'Calibri','font_size': 11,\n",
        "                         'align': 'center','valign': 'vcenter','underline': 33})\n",
        "    f_label = wb.add_format({'bold': True,'italic': True,'align':'left','valign':'vcenter','font_name':'Calibri','font_size':11})\n",
        "    f_left  = wb.add_format({'align':'left','valign':'vcenter','font_name':'Calibri','font_size':11})\n",
        "    f_date  = wb.add_format({'num_format':'m/d/yyyy','align':'left','valign':'vcenter','font_name':'Calibri','font_size':11})\n",
        "    f_tenor = wb.add_format({'num_format':'0.0\"y\"','align':'left','valign':'vcenter','font_name':'Calibri','font_size':11})\n",
        "    f_cpn   = wb.add_format({'num_format':'0.000','align':'left','valign':'vcenter','font_name':'Calibri','font_size':11})\n",
        "    f_amt   = wb.add_format({'num_format':'#,##0.00\" B\"','align':'left','valign':'vcenter','font_name':'Calibri','font_size':11})\n",
        "    f_tspr  = wb.add_format({'num_format':'0.0','align':'left','valign':'vcenter','font_name':'Calibri','font_size':11})\n",
        "    f_tgt_left = wb.add_format({'bold': True,'bg_color':'#DCE6F1','align':'left','valign':'vcenter','font_name':'Calibri','font_size':11})\n",
        "    f_tgt_date = wb.add_format({'bold': True,'bg_color':'#DCE6F1','num_format':'m/d/yyyy','align':'left','valign':'vcenter','font_name':'Calibri','font_size':11})\n",
        "    f_tgt_ten  = wb.add_format({'bold': True,'bg_color':'#DCE6F1','num_format':'0.0\"y\"','align':'left','valign':'vcenter','font_name':'Calibri','font_size':11})\n",
        "    f_tgt_cpn  = wb.add_format({'bold': True,'bg_color':'#DCE6F1','num_format':'0.000','align':'left','valign':'vcenter','font_name':'Calibri','font_size':11})\n",
        "    f_tgt_amt  = wb.add_format({'bold': True,'bg_color':'#DCE6F1','num_format':'#,##0.00\" B\"','align':'left','valign':'vcenter','font_name':'Calibri','font_size':11})\n",
        "    f_tgt_tspr = wb.add_format({'bold': True,'bg_color':'#DCE6F1','num_format':'0.0','align':'left','valign':'vcenter','font_name':'Calibri','font_size':11})\n",
        "    widths = [30,28,10,8,10,12,9,12,8,8,12,9]\n",
        "    start_row, start_col = 2, 2\n",
        "    for j in range(len(cols)): ws.write_blank(start_row, start_col + j, None, prehdr)\n",
        "    for j, c in enumerate(cols): ws.write(start_row + 1, start_col + j, c, hdr)\n",
        "    for j, w in enumerate(widths): ws.set_column(start_col + j, start_col + j, w)\n",
        "    target_by_ust_norm = {norm_ust(k): v for k, v in (target_by_ust or {}).items()}\n",
        "    r = start_row + 2 + 1\n",
        "    for ust, g in df.groupby(\"UST\", sort=False):\n",
        "        g = g.copy()\n",
        "        g[\"__canon\"] = None\n",
        "        for i, rr in g.iterrows():\n",
        "            parsed = parse_bond_label(str(rr[\"Bond\"]))\n",
        "            g.at[i,\"__canon\"] = canonical_label(parsed) if parsed else None\n",
        "        target_canon = target_by_ust_norm.get(ust)\n",
        "        if target_canon and target_canon in set(g[\"__canon\"].dropna()):\n",
        "            tgt_df = g[g[\"__canon\"] == target_canon]\n",
        "            peers  = g[g[\"__canon\"] != target_canon].sort_values([\"Tenor\",\"Issuer\"], ascending=[True, True])\n",
        "            g = pd.concat([tgt_df, peers], ignore_index=True)\n",
        "        else:\n",
        "            g = g.sort_values([\"Tenor\",\"Issuer\"], ascending=[True, True]).reset_index(drop=True)\n",
        "        ws.set_row(r, 3.0); r += 1\n",
        "        ws.write(r, start_col + 0, ust, f_label); r += 1\n",
        "        ws.set_row(r, 3.0); r += 1\n",
        "        first_row = r; last_row = r\n",
        "        for _, rr in g.iterrows():\n",
        "            is_tgt = (rr[\"__canon\"] == target_canon)\n",
        "            L, D, Tn, Cp, Am, Ts = (\n",
        "                (f_tgt_left, f_tgt_date, f_tgt_ten, f_tgt_cpn, f_tgt_amt, f_tgt_tspr)\n",
        "                if is_tgt else\n",
        "                (f_left, f_date, f_tenor, f_cpn, f_amt, f_tspr)\n",
        "            )\n",
        "            c = start_col\n",
        "            ws.write(r, c+0, rr.get(\"Issuer\",\"\"), L)\n",
        "            ws.write(r, c+1, rr.get(\"Bond\",\"\"),   L)\n",
        "            ws.write(r, c+2, rr.get(\"Seniority\",\"\"), L)\n",
        "            ws.write(r, c+3, rr.get(\"Moody's\",\"\"),   L)\n",
        "            ws.write(r, c+4, rr.get(\"S&P Rating\",\"\"),L)\n",
        "            idt, mdt = rr.get(\"Issue Date\"), rr.get(\"Maturity\")\n",
        "            if pd.notna(idt): ws.write_datetime(r, c+5, datetime.combine(idt, dtime.min), D)\n",
        "            else:             ws.write(r, c+5, \"\", D)\n",
        "            ten = rr.get(\"Tenor\")\n",
        "            if pd.notna(ten): ws.write_number(r, c+6, float(ten), Tn)\n",
        "            else:             ws.write(r, c+6, \"\", Tn)\n",
        "            if pd.notna(mdt): ws.write_datetime(r, c+7, datetime.combine(mdt, dtime.min), D)\n",
        "            else:             ws.write(r, c+7, \"\", D)\n",
        "            ws.write(r, c+8, rr.get(\"UST\",\"\"), L)\n",
        "            coup = rr.get(\"Coupon\")\n",
        "            if pd.notna(coup): ws.write_number(r, c+9, float(coup), Cp)\n",
        "            else:              ws.write(r, c+9, \"\", Cp)\n",
        "            amt = rr.get(\"Quantity\")\n",
        "            if pd.notna(amt): ws.write_number(r, c+10, float(amt)/1_000_000_000.0, f_amt)\n",
        "            else:             ws.write(r, c+10, \"\", f_amt)\n",
        "            ts = rr.get(\"T-Spread\")\n",
        "            if pd.notna(ts):  ws.write_number(r, c+11, float(ts), f_tspr)\n",
        "            else:             ws.write(r, c+11, \"\", f_tspr)\n",
        "            last_row = r\n",
        "            r += 1\n",
        "        stripe_fmt = wb.add_format({'bg_color':'#F2F2F2'})\n",
        "        ws.conditional_format(first_row, start_col, last_row, start_col + len(cols) - 1, {\n",
        "            'type': 'formula',\n",
        "            'criteria': f'=MOD(ROW()-{first_row},2)=0',\n",
        "            'format': stripe_fmt\n",
        "        })\n",
        "\n",
        "def build_target_workbook(bond_table_df: pd.DataFrame, adjusted_df: pd.DataFrame) -> str | None:\n",
        "    global TARGET_XLSX\n",
        "    if adjusted_df.empty or adjusted_df.shape[1] < 2:\n",
        "        print(\"âš ï¸  No Adjusted data found; cannot build Target workbook.\")\n",
        "        return None\n",
        "    ticker = input(\"Enter the target TICKER (e.g., JPM): \").strip().upper()\n",
        "    if not ticker:\n",
        "        print(\"âš ï¸  No ticker provided; skipping Target.\")\n",
        "        return None\n",
        "    canon_issue_map, canon_ust_map = _maps_from_bond_table(bond_table_df)\n",
        "    by_bucket = {}\n",
        "    for canon in adjusted_df.columns[1:]:\n",
        "        m = re.match(r\"^\\s*([A-Z0-9\\.\\-]+)\", str(canon))\n",
        "        if not m: continue\n",
        "        if m.group(1) != ticker: continue\n",
        "        u = canon_ust_map.get(canon)\n",
        "        if u: by_bucket.setdefault(norm_ust(u), []).append(canon)\n",
        "    if not by_bucket:\n",
        "        print(f\"âš ï¸  No bonds with UST buckets found for '{ticker}'.\")\n",
        "        return None\n",
        "    def _pick_latest_issue(canon_list):\n",
        "        if not canon_list: return None\n",
        "        with_dates = [(c, canon_issue_map.get(c)) for c in canon_list]\n",
        "        dated = [t for t in with_dates if t[1] is not None]\n",
        "        if dated: return max(dated, key=lambda t: (t[1], t[0]))[0]\n",
        "        def _cp(c):\n",
        "            m = re.match(r\"^\\s*([A-Z0-9\\.\\-]+)\\s+(\\d+(?:\\.\\d+)?)%\", c)\n",
        "            return float(m.group(2)) if m else -1.0\n",
        "        return max(canon_list, key=lambda c: (_cp(c), c))\n",
        "    target_by_ust = {}\n",
        "    for u, cands in by_bucket.items():\n",
        "        tcanon = _pick_latest_issue(cands)\n",
        "        if tcanon: target_by_ust[u] = tcanon\n",
        "    TARGET_XLSX = os.path.join(OUTPUT_DIR, f\"Target {FS_DATE}.xlsx\")\n",
        "    with pd.ExcelWriter(TARGET_XLSX, engine=\"xlsxwriter\", datetime_format=\"yyyy-mm-dd\") as tw:\n",
        "        _write_bond_table_target(tw, bond_table_df, target_by_ust)\n",
        "        def m_order(txt):\n",
        "            s = norm_ust(txt)\n",
        "            if s == \"Perp\": return 10**9\n",
        "            m = re.match(r\"(\\d+)\", s);  return int(m.group(1)) if m else 10**8\n",
        "        for ust, cands in sorted(by_bucket.items(), key=lambda kv: m_order(kv[0])):\n",
        "            tcanon = target_by_ust.get(ust)\n",
        "            if not tcanon: continue\n",
        "            peers_all = []\n",
        "            for c in adjusted_df.columns[1:]:\n",
        "                if c == tcanon: continue\n",
        "                if norm_ust(_maps_from_bond_table(bond_table_df)[1].get(c)) == ust:\n",
        "                    peers_all.append(c)\n",
        "            peers_by_ticker = defaultdict(list)\n",
        "            for c in peers_all:\n",
        "                m = re.match(r\"^\\s*([A-Z0-9\\.\\-]+)\", str(c))\n",
        "                if m: peers_by_ticker[m.group(1)].append(c)\n",
        "            peer_list = []\n",
        "            for _, lst in peers_by_ticker.items():\n",
        "                keep = _pick_latest_issue(lst)\n",
        "                if keep and keep != tcanon: peer_list.append(keep)\n",
        "            out = pd.DataFrame({\"Date\": adjusted_df[\"Date\"]})\n",
        "            out[tcanon] = pd.to_numeric(adjusted_df.get(tcanon), errors=\"coerce\")\n",
        "            peer_cols = []\n",
        "            for i, c in enumerate(sorted(peer_list), start=1):\n",
        "                name = f\"Peer Bond #{i} â€” {c}\"\n",
        "                out[name] = pd.to_numeric(adjusted_df.get(c), errors=\"coerce\")\n",
        "                peer_cols.append(name)\n",
        "            if peer_cols:\n",
        "                cnt = out[peer_cols].notna().sum(axis=1)\n",
        "                out[\"Peer Avg\"] = out[peer_cols].mean(axis=1)\n",
        "                out.loc[cnt < 2, \"Peer Avg\"] = np.nan\n",
        "            else:\n",
        "                out[\"Peer Avg\"] = np.nan\n",
        "            disp_target_col = f\"Target Bond â€” {tcanon}\"\n",
        "            out = out.rename(columns={tcanon: disp_target_col})\n",
        "            start_d, end_d = pick_month_window(out[\"Date\"], out[disp_target_col], out[\"Peer Avg\"])\n",
        "            mask = (out[\"Date\"] >= start_d) & (out[\"Date\"] <= end_d)\n",
        "            out_win = out.loc[mask].reset_index(drop=True)\n",
        "            sheet_name = ust\n",
        "            out_win.to_excel(tw, sheet_name=sheet_name, index=False, startrow=29, startcol=0)\n",
        "            wb, ws = tw.book, tw.sheets[sheet_name]\n",
        "            nrows = len(out_win)\n",
        "            tgt_idx = out_win.columns.get_loc(disp_target_col)\n",
        "            pav_idx = out_win.columns.get_loc(\"Peer Avg\")\n",
        "            peer_idx = [out_win.columns.get_loc(c) for c in peer_cols]\n",
        "            def add_series(chart, col_idx, name, is_target=False, width=2.0):\n",
        "                chart.add_series({\n",
        "                    'name': name,\n",
        "                    'categories': [sheet_name, 30, 0, 30 + nrows - 1, 0],\n",
        "                    'values':     [sheet_name, 30, col_idx, 30 + nrows - 1, col_idx],\n",
        "                    'line': {'color': 'red' if is_target else None, 'width': width}\n",
        "                })\n",
        "            y_pg_min, y_pg_max = y_range_5s_push10(\n",
        "                np.concatenate([pd.to_numeric(out_win[disp_target_col], errors=\"coerce\").values] +\n",
        "                               [pd.to_numeric(out_win[c], errors=\"coerce\").values for c in peer_cols])\n",
        "            )\n",
        "            y_pa_min, y_pa_max = y_range_5s_push10(\n",
        "                np.concatenate([pd.to_numeric(out_win[disp_target_col], errors=\"coerce\").values,\n",
        "                                pd.to_numeric(out_win[\"Peer Avg\"], errors=\"coerce\").values])\n",
        "            )\n",
        "            ch_pg = base_chart(wb)\n",
        "            configure_date_axis_monthly(ch_pg, start_d, end_d)\n",
        "            ch_pg.set_y_axis({'min': y_pg_min, 'max': y_pg_max, 'label_position': 'low',\n",
        "                              'major_unit': 10, 'major_gridlines': {'visible': False},\n",
        "                              'minor_gridlines': {'visible': False}})\n",
        "            for idx in peer_idx:\n",
        "                pretty = out_win.columns[idx].split('â€”', 1)[-1].strip() if 'â€”' in out_win.columns[idx] else out_win.columns[idx]\n",
        "                add_series(ch_pg, idx, pretty, False, 2.0)\n",
        "            add_series(ch_pg, tgt_idx, disp_target_col.replace(\"Target Bond â€” \", \"\"), True, 3.0)\n",
        "            ws.insert_chart(1, 0, ch_pg, {'x_scale': 1.1, 'y_scale': 1.0})\n",
        "            ch_pa = base_chart(wb)\n",
        "            configure_date_axis_monthly(ch_pa, start_d, end_d)\n",
        "            ch_pa.set_y_axis({'min': y_pa_min, 'max': y_pa_max, 'label_position': 'low',\n",
        "                              'major_unit': 10, 'major_gridlines': {'visible': False},\n",
        "                              'minor_gridlines': {'visible': False}})\n",
        "            add_series(ch_pa, pav_idx, \"Peer Avg\", False, 2.0)\n",
        "            add_series(ch_pa, tgt_idx, disp_target_col.replace(\"Target Bond â€” \", \"\"), True, 2.0)\n",
        "            ws.insert_chart(12, 0, ch_pa, {'x_scale': 1.1, 'y_scale': 1.0})\n",
        "    print(f\"ðŸ“˜ Wrote: {TARGET_XLSX}\")\n",
        "    return TARGET_XLSX\n",
        "\n",
        "# =========================\n",
        "# EVENT ANALYSIS workbook â€” FIXED date windows per bond\n",
        "# =========================\n",
        "def _write_event_bond_table(writer, bond_table_df: pd.DataFrame, selected_canons: list[str]):\n",
        "    \"\"\"Flat Bond Table for Event Analysis; bold UST column; even-row striping.\"\"\"\n",
        "    cols = [\"Issuer\",\"Bond\",\"Seniority\",\"Moody's\",\"S&P Rating\",\"Issue Date\",\"Tenor\",\n",
        "            \"Maturity\",\"UST\",\"Coupon\",\"Quantity\",\"T-Spread\"]\n",
        "    df = bond_table_df.copy().rename(columns={\"Amount Issued\":\"Quantity\"})\n",
        "    # filter to selected\n",
        "    def to_canon(s):\n",
        "        parsed = parse_bond_label(str(s));  return canonical_label(parsed) if parsed else None\n",
        "    df[\"__canon\"] = df[\"Bond\"].apply(to_canon)\n",
        "    df = df[df[\"__canon\"].isin(set(selected_canons))].copy()\n",
        "    # types\n",
        "    df[\"Issue Date\"] = pd.to_datetime(df[\"Issue Date\"], errors=\"coerce\").dt.date\n",
        "    df[\"Maturity\"]   = pd.to_datetime(df[\"Maturity\"], errors=\"coerce\").dt.date\n",
        "    df[\"Tenor\"]      = pd.to_numeric(df[\"Tenor\"], errors=\"coerce\")\n",
        "    for c in [\"Coupon\",\"Quantity\",\"T-Spread\"]: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    # recompute Bond display & UST\n",
        "    def bond_left(s):\n",
        "        s = str(s) if pd.notna(s) else \"\"\n",
        "        m = re.search(r\"%\", s);  return s[:m.end()] if m else s.strip()\n",
        "    def fmt_m_yyyy(d):  return \"\" if pd.isna(d) or d is None else f\"{d.month}/{d.year}\"\n",
        "    df[\"Bond\"] = [f\"{bond_left(b)} {fmt_m_yyyy(m)}\".strip() for b, m in zip(df[\"Bond\"], df[\"Maturity\"])]\n",
        "    df[\"UST\"] = df[\"UST\"].apply(norm_ust)\n",
        "    # sort\n",
        "    df.sort_values(by=[\"Tenor\",\"Issuer\"], ascending=[True, True], inplace=True)\n",
        "    df = df[cols]\n",
        "    # write\n",
        "    wb = writer.book;  ws = wb.add_worksheet(\"Bond Table\");  ws.hide_gridlines(2)\n",
        "    blue = \"#1F497D\"\n",
        "    prehdr = wb.add_format({'bg_color': blue})\n",
        "    hdr = wb.add_format({'bold': True,'font_color': 'white','bg_color': blue,\n",
        "                         'font_name': 'Calibri','font_size': 11,'align':'center','valign':'vcenter','underline':33})\n",
        "    f_left   = wb.add_format({'align':'left','valign':'vcenter','font_name':'Calibri','font_size':11})\n",
        "    f_left_b = wb.add_format({'align':'left','valign':'vcenter','font_name':'Calibri','font_size':11,'bold':True})\n",
        "    f_date   = wb.add_format({'num_format':'m/d/yyyy','align':'left','valign':'vcenter','font_name':'Calibri','font_size':11})\n",
        "    f_tenor  = wb.add_format({'num_format':'0.0\"y\"','align':'left','valign':'vcenter','font_name':'Calibri','font_size':11})\n",
        "    f_cpn    = wb.add_format({'num_format':'0.000','align':'left','valign':'vcenter','font_name':'Calibri','font_size':11})\n",
        "    f_amt    = wb.add_format({'num_format':'#,##0.00\" B\"','align':'left','valign':'vcenter','font_name':'Calibri','font_size':11})\n",
        "    f_tspr   = wb.add_format({'num_format':'0.0','align':'left','valign':'vcenter','font_name':'Calibri','font_size':11})\n",
        "    widths = [30,28,10,8,10,12,9,12,8,8,12,9]\n",
        "    start_row, start_col = 2, 2\n",
        "    for j in range(len(cols)): ws.write_blank(start_row, start_col + j, None, prehdr)\n",
        "    for j, c in enumerate(cols): ws.write(start_row + 1, start_col + j, c, hdr)\n",
        "    for j, w in enumerate(widths): ws.set_column(start_col + j, start_col + j, w)\n",
        "    r = start_row + 2 + 1\n",
        "    ws.set_row(r, 3.0);  r += 1  # one pad row\n",
        "    first_data_row = r;  last_data_row = r-1\n",
        "    for _, rr in df.iterrows():\n",
        "        c = start_col\n",
        "        ws.write(r, c+0, rr.get(\"Issuer\",\"\"), f_left)\n",
        "        ws.write(r, c+1, rr.get(\"Bond\",\"\"),   f_left)\n",
        "        ws.write(r, c+2, rr.get(\"Seniority\",\"\"), f_left)\n",
        "        ws.write(r, c+3, rr.get(\"Moody's\",\"\"),   f_left)\n",
        "        ws.write(r, c+4, rr.get(\"S&P Rating\",\"\"),f_left)\n",
        "        idt, mdt = rr.get(\"Issue Date\"), rr.get(\"Maturity\")\n",
        "        if pd.notna(idt): ws.write_datetime(r, c+5, datetime.combine(idt, dtime.min), f_date)\n",
        "        else:             ws.write(r, c+5, \"\", f_date)\n",
        "        ten = rr.get(\"Tenor\")\n",
        "        if pd.notna(ten): ws.write_number(r, c+6, float(ten), f_tenor)\n",
        "        else:             ws.write(r, c+6, \"\", f_tenor)\n",
        "        if pd.notna(mdt): ws.write_datetime(r, c+7, datetime.combine(mdt, dtime.min), f_date)\n",
        "        else:             ws.write(r, c+7, \"\", f_date)\n",
        "        ws.write(r, c+8, rr.get(\"UST\",\"\"), f_left_b)  # bold UST\n",
        "        coup = rr.get(\"Coupon\")\n",
        "        if pd.notna(coup): ws.write_number(r, c+9, float(coup), f_cpn)\n",
        "        else:              ws.write(r, c+9, \"\", f_cpn)\n",
        "        amt = rr.get(\"Quantity\")\n",
        "        if pd.notna(amt): ws.write_number(r, c+10, float(amt)/1_000_000_000.0, f_amt)\n",
        "        else:             ws.write(r, c+10, \"\", f_amt)\n",
        "        ts = rr.get(\"T-Spread\")\n",
        "        if pd.notna(ts):  ws.write_number(r, c+11, float(ts), f_tspr)\n",
        "        else:             ws.write(r, c+11, \"\", f_tspr)\n",
        "        last_data_row = r;  r += 1\n",
        "    stripe_fmt = wb.add_format({'bg_color':'#F2F2F2'})\n",
        "    ws.conditional_format(first_data_row, start_col, last_data_row, start_col + len(cols) - 1, {\n",
        "        'type': 'formula',\n",
        "        'criteria': f'=MOD(ROW()-{first_data_row},2)=0',\n",
        "        'format': stripe_fmt\n",
        "    })\n",
        "\n",
        "def build_event_analysis_workbook(bond_table_df: pd.DataFrame, adjusted_df: pd.DataFrame) -> str | None:\n",
        "    global EVENT_XLSX\n",
        "    if adjusted_df.empty or adjusted_df.shape[1] < 2:\n",
        "        print(\"âš ï¸  No Adjusted data; cannot build Event Analysis.\")\n",
        "        return None\n",
        "\n",
        "    ticker = input(\"Enter the issuer TICKER for Event Analysis (e.g., JPM): \").strip().upper()\n",
        "    if not ticker:\n",
        "        print(\"âš ï¸  No ticker provided; skipping Event Analysis.\")\n",
        "        return None\n",
        "\n",
        "    event_date_str = input(\"Enter the EVENT DATE (e.g., 6/15/2025): \").strip()\n",
        "    try:\n",
        "        event_date = pd.to_datetime(event_date_str, errors=\"raise\").date()\n",
        "    except Exception:\n",
        "        print(\"â— Could not parse that date. Use m/d/yyyy like 6/15/2025.\")\n",
        "        return None\n",
        "\n",
        "    # maps\n",
        "    canon_issue_map, canon_ust_map = _maps_from_bond_table(bond_table_df)\n",
        "\n",
        "    # discover bonds of ticker; select latest per UST\n",
        "    by_bucket = {}\n",
        "    for canon in adjusted_df.columns[1:]:\n",
        "        m = re.match(r\"^\\s*([A-Z0-9\\.\\-]+)\", str(canon))\n",
        "        if not m: continue\n",
        "        if m.group(1) != ticker: continue\n",
        "        u = canon_ust_map.get(canon)\n",
        "        if u: by_bucket.setdefault(norm_ust(u), []).append(canon)\n",
        "\n",
        "    if not by_bucket:\n",
        "        print(f\"âš ï¸  No bonds for ticker '{ticker}' found in Adjusted.\")\n",
        "        return None\n",
        "\n",
        "    def _pick_latest_issue(canon_list):\n",
        "        if not canon_list: return None\n",
        "        with_dates = [(c, canon_issue_map.get(c)) for c in canon_list]\n",
        "        dated = [t for t in with_dates if t[1] is not None]\n",
        "        if dated: return max(dated, key=lambda t: (t[1], t[0]))[0]\n",
        "        # fallback: highest coupon\n",
        "        def _cp(c):\n",
        "            m = re.match(r\"^\\s*([A-Z0-9\\.\\-]+)\\s+(\\d+(?:\\.\\d+)?)%\", c)\n",
        "            return float(m.group(2)) if m else -1.0\n",
        "        return max(canon_list, key=lambda c: (_cp(c), c))\n",
        "\n",
        "    selected = []\n",
        "    for u, cands in by_bucket.items():\n",
        "        tcanon = _pick_latest_issue(cands)\n",
        "        if tcanon: selected.append((u, tcanon))\n",
        "\n",
        "    if not selected:\n",
        "        print(\"âš ï¸  No bonds selected for Event Analysis.\")\n",
        "        return None\n",
        "\n",
        "    EVENT_XLSX = os.path.join(OUTPUT_DIR, f\"Event Analysis {FS_DATE}.xlsx\")\n",
        "    with pd.ExcelWriter(EVENT_XLSX, engine=\"xlsxwriter\", datetime_format=\"yyyy-mm-dd\") as tw:\n",
        "        wb = tw.book\n",
        "\n",
        "        # 0) Flat Bond Table\n",
        "        _write_event_bond_table(tw, bond_table_df, [canon for _, canon in selected])\n",
        "\n",
        "        # 1..n) One sheet per bond â€” UST-named tabs (unique)\n",
        "        used_names = set()\n",
        "        for idx, (ust, canon) in enumerate(sorted(selected, key=lambda t: (999999 if t[0]==\"Perp\" else int(t[0].replace(\"y\",\"\"))))):\n",
        "            base_name = norm_ust(ust) if ust else \"Bond\"\n",
        "            sheet_name = base_name\n",
        "            k = 2\n",
        "            while sheet_name in used_names:\n",
        "                sheet_name = f\"{base_name}-{k}\"; k += 1\n",
        "            used_names.add(sheet_name)\n",
        "\n",
        "            # Build per-bond frame from Adjusted\n",
        "            dates = pd.to_datetime(adjusted_df[\"Date\"], errors=\"coerce\").dt.date\n",
        "            vals  = pd.to_numeric(adjusted_df.get(canon), errors=\"coerce\")\n",
        "            df = pd.DataFrame({\"Date\": dates, \"T-Spread\": vals})\n",
        "            df[\"Prior\"] = df[\"T-Spread\"]\n",
        "            df.loc[df[\"Date\"] >= event_date, \"Prior\"] = np.nan\n",
        "            df[\"Post\"]  = df[\"T-Spread\"]\n",
        "            df.loc[df[\"Date\"] < event_date, \"Post\"] = np.nan\n",
        "\n",
        "            # ---- Compute bond-specific first/last non-NaN dates ----\n",
        "            valid_mask = df[\"T-Spread\"].notna().values\n",
        "            if np.any(valid_mask):\n",
        "                inds = np.flatnonzero(valid_mask)\n",
        "                bond_first_date = df[\"Date\"].iloc[inds[0]]\n",
        "                bond_last_date  = df[\"Date\"].iloc[inds[-1]]\n",
        "            else:\n",
        "                bond_first_date = event_date\n",
        "                bond_last_date  = event_date\n",
        "\n",
        "            # ---- Write table (charts at top, table starts row 30) ----\n",
        "            start_row = 29\n",
        "            df.to_excel(tw, sheet_name=sheet_name, index=False, startrow=start_row, startcol=0)\n",
        "            ws = tw.sheets[sheet_name]\n",
        "            # put canonical label + bold UST at top\n",
        "            ws.write(0, 0, canon)\n",
        "            bold_fmt = wb.add_format({'bold': True})\n",
        "            ws.write(0, 1, base_name, bold_fmt)\n",
        "\n",
        "            nrows = len(df)\n",
        "            y_min, y_max = y_range_5s_push10(df[\"T-Spread\"].values)\n",
        "            color = EA_COLORS[idx % len(EA_COLORS)]\n",
        "\n",
        "            # ---- Chart 1: All data split by event (bond-specific x range) ----\n",
        "            ch1 = base_chart(wb)\n",
        "            configure_date_axis_monthly(ch1, bond_first_date, bond_last_date)\n",
        "            ch1.set_y_axis({'min': y_min, 'max': y_max, 'label_position': 'low',\n",
        "                            'major_unit': 10, 'major_gridlines': {'visible': False},\n",
        "                            'minor_gridlines': {'visible': False}})\n",
        "            # Prior: solid line\n",
        "            ch1.add_series({\n",
        "              'name': f\"Prior-{canon}\",\n",
        "              'categories': [sheet_name, start_row+1, 0, start_row+nrows, 0],\n",
        "              'values':     [sheet_name, start_row+1, 2, start_row+nrows, 2],\n",
        "              'line': {'dash_type': 'solid', 'width': 2.0, 'color': color},\n",
        "              'marker': {'type': 'none'}\n",
        "            })\n",
        "\n",
        "            # Post: square-dot dashed line (same color as Prior), NOT markers-only\n",
        "            ch1.add_series({\n",
        "              'name': f\"Post-{canon}\",\n",
        "              'categories': [sheet_name, start_row+1, 0, start_row+nrows, 0],\n",
        "              'values':     [sheet_name, start_row+1, 3, start_row+nrows, 3],\n",
        "              'line': {'dash_type': 'square_dot', 'width': 2.0, 'color': color},\n",
        "              'marker': {'type': 'none'}\n",
        "            })\n",
        "\n",
        "            ws.insert_chart(1, 0, ch1, {'x_scale': 1.1, 'y_scale': 1.0})\n",
        "\n",
        "            # ---- Chart 2: Post-only (bond-specific x range) ----\n",
        "            ch2 = base_chart(wb)\n",
        "            # first post date for this bond = first non-NaN on/after event_date\n",
        "            post_mask = (df[\"Date\"] >= event_date) & df[\"T-Spread\"].notna()\n",
        "            if post_mask.any():\n",
        "                post_first_date = df.loc[post_mask, \"Date\"].iloc[0]\n",
        "                post_last_date  = df.loc[df[\"T-Spread\"].notna(), \"Date\"].iloc[-1]\n",
        "            else:\n",
        "                post_first_date = event_date\n",
        "                post_last_date  = bond_last_date\n",
        "            configure_date_axis_monthly(ch2, post_first_date, post_last_date)\n",
        "            ch2.set_y_axis({'min': y_min, 'max': y_max, 'label_position': 'low',\n",
        "                            'major_unit': 10, 'major_gridlines': {'visible': False},\n",
        "                            'minor_gridlines': {'visible': False}})\n",
        "            # locate first row index >= event_date (even if NaNs, we start from nearest)\n",
        "            first_post_idx = df.index[df[\"Date\"] >= post_first_date]\n",
        "            if len(first_post_idx) == 0:\n",
        "                post_start_row = start_row+1\n",
        "            else:\n",
        "                post_start_row = start_row + 1 + int(first_post_idx[0])\n",
        "            post_end_row = start_row + nrows\n",
        "            ch2.add_series({\n",
        "                'name': canon,\n",
        "                'categories': [sheet_name, post_start_row, 0, post_end_row, 0],\n",
        "                'values':     [sheet_name, post_start_row, 1, post_end_row, 1],\n",
        "                'line': {'width': 2.0, 'color': color},\n",
        "                'marker': {'type': 'none'}\n",
        "            })\n",
        "            ws.insert_chart(12, 0, ch2, {'x_scale': 1.1, 'y_scale': 1.0})\n",
        "\n",
        "    print(f\"ðŸ“˜ Wrote: {EVENT_XLSX}\")\n",
        "    return EVENT_XLSX\n",
        "\n",
        "# =========================\n",
        "# Runner\n",
        "# =========================\n",
        "if __name__ == \"__main__\":\n",
        "    raw_dir = get_raw_folder()\n",
        "    print(f\"ðŸ“‚ Scanning: {raw_dir}\")\n",
        "    bond_table_df, adjusted_df, trace_df = ingest_bonds_and_history(raw_dir)\n",
        "\n",
        "    # Build Cleaned\n",
        "    build_cleaned_workbook(bond_table_df, adjusted_df, trace_df)\n",
        "\n",
        "    # Build Target?\n",
        "    if input(\"\\nBuild TARGET workbook? (y/n): \").strip().lower() in {\"y\",\"yes\"}:\n",
        "        build_target_workbook(bond_table_df, adjusted_df)\n",
        "\n",
        "    # Build Event Analysis?\n",
        "    if input(\"\\nBuild EVENT ANALYSIS workbook? (y/n): \").strip().lower() in {\"y\",\"yes\"}:\n",
        "        build_event_analysis_workbook(bond_table_df, adjusted_df)\n",
        "\n",
        "    print(f\"\\nðŸ“‚ Output directory: {os.path.abspath(OUTPUT_DIR)}\")\n",
        "\n",
        "    # ---- Colab auto-downloads (safe, prints what was queued) ----\n",
        "    try:\n",
        "        from google.colab import files as colab_files  # type: ignore\n",
        "        print(\"\\nâ¬‡ï¸  Prompting download dialogs...\")\n",
        "        downloaded = []\n",
        "        for varname in [\"OUT_XLSX\", \"TARGET_XLSX\", \"EVENT_XLSX\"]:\n",
        "            try:\n",
        "                if varname in globals() and globals()[varname]:\n",
        "                    colab_files.download(globals()[varname])\n",
        "                    downloaded.append(globals()[varname])\n",
        "            except Exception:\n",
        "                pass\n",
        "        if downloaded:\n",
        "            print(\"\\nðŸ“˜ Successfully queued for download:\")\n",
        "            for f in downloaded:\n",
        "                print(\"  -\", f)\n",
        "        else:\n",
        "            print(\"\\nâš ï¸ No workbooks available to download.\")\n",
        "    except Exception:\n",
        "        # Not in Colab; just print paths\n",
        "        print(\"\\n(Info) Not in Colab; skipping auto-downloads.\")\n",
        "        if OUT_XLSX:    print(\"  Cleaned:\", OUT_XLSX)\n",
        "        if TARGET_XLSX: print(\"  Target: \", TARGET_XLSX)\n",
        "        if EVENT_XLSX:  print(\"  Event:  \", EVENT_XLSX)\n",
        "\n",
        "    print(\"\\nðŸŽ¯ Done (v4.3.1).\")\n"
      ]
    }
  ]
}